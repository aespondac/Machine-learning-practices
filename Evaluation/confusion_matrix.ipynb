{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of data for which the true values are known.\n",
    "\n",
    "During this notebook, we will see how get the data from the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy of the model\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "# Calculate Error of the model \n",
    "\n",
    "def error(y_true, y_pred):\n",
    "    error = np.sum(y_true != y_pred) / len(y_true)\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Confusion Matrix of the two classes model\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    true_positive = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    true_negative = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    false_positive = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    false_negative = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return np.array([[true_negative, false_positive], [false_negative, true_positive]])\n",
    "\n",
    "# Calculate Precision of the model\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    precision = cm[1, 1] / (cm[1, 1] + cm[0, 1])\n",
    "    return precision\n",
    "\n",
    "# Calculate Recall of the model\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    recall = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "    return recall\n",
    "\n",
    "#Prositive Predictive Value\n",
    "\n",
    "def positive_predictive_value(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    ppv = cm[1, 1] / (cm[1, 1] + cm[0, 1])\n",
    "    return ppv\n",
    "\n",
    "#True Positive Rate\n",
    "\n",
    "def true_positive_rate(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tpr = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "    return tpr\n",
    "\n",
    "#True Negative Rate\n",
    "\n",
    "def true_negative_rate(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tnr = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "    return tnr\n",
    "\n",
    "#False Positive Rate\n",
    "\n",
    "def false_positive_rate(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fpr = cm[0, 1] / (cm[0, 1] + cm[0, 0])\n",
    "    return fpr\n",
    "\n",
    "#False Negative Rate\n",
    "\n",
    "def false_negative_rate(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fnr = cm[1, 0] / (cm[1, 0] + cm[1, 1])\n",
    "    return fnr\n",
    "\n",
    "#F1 Score\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision_score = precision(y_true, y_pred)\n",
    "    recall_score = recall(y_true, y_pred)\n",
    "    f1_score = 2 * (precision_score * recall_score) / (precision_score + recall_score)\n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Testing the functions ----------\n",
      "Accuracy:  0.9590643274853801\n",
      "Error:  0.04093567251461988\n",
      "Confusion Matrix:  [[ 62   1]\n",
      " [  6 102]]\n",
      "Precision:  0.9902912621359223\n",
      "Recall:  0.9444444444444444\n",
      "Positive Predictive Value:  0.9902912621359223\n",
      "True Positive Rate:  0.9444444444444444\n",
      "True Negative Rate:  0.9841269841269841\n",
      "False Positive Rate:  0.015873015873015872\n",
      "False Negative Rate:  0.05555555555555555\n",
      "F1 Score:  0.966824644549763\n",
      "---------- Testing sklearn.metrics functions ----------\n",
      "Accuracy:  0.9590643274853801\n",
      "Error:  0.040935672514619936\n",
      "Confusion Matrix:  [[ 62   1]\n",
      " [  6 102]]\n",
      "Precision:  0.9902912621359223\n",
      "Recall:  0.9444444444444444\n",
      "Positive Predictive Value:  0.9902912621359223\n",
      "True Positive Rate:  0.9444444444444444\n",
      "True Negative Rate: Not directly available in sklearn.metrics\n",
      "False Positive Rate: Not directly available in sklearn.metrics\n",
      "False Negative Rate:  0.05555555555555558\n",
      "F1 Score:  0.966824644549763\n"
     ]
    }
   ],
   "source": [
    "# Test funcionality of the functions\n",
    "\n",
    "#importing the dataset \n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "model = LogisticRegression(max_iter=200000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Testing the functions\n",
    "print(\"---------- Testing the functions ----------\")\n",
    "print(\"Accuracy: \", accuracy(y_test, y_pred))\n",
    "print(\"Error: \", error(y_test, y_pred))\n",
    "print(\"Confusion Matrix: \", confusion_matrix(y_test, y_pred))\n",
    "print(\"Precision: \", precision(y_test, y_pred))\n",
    "print(\"Recall: \", recall(y_test, y_pred))\n",
    "print(\"Positive Predictive Value: \", positive_predictive_value(y_test, y_pred))\n",
    "print(\"True Positive Rate: \", true_positive_rate(y_test, y_pred))\n",
    "print(\"True Negative Rate: \", true_negative_rate(y_test, y_pred))\n",
    "print(\"False Positive Rate: \", false_positive_rate(y_test, y_pred))\n",
    "print(\"False Negative Rate: \", false_negative_rate(y_test, y_pred))\n",
    "print(\"F1 Score: \", f1_score(y_test, y_pred))\n",
    "\n",
    "# Importing the metrics from sklearn\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "print(\"---------- Testing sklearn.metrics functions ----------\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Error: \", 1 - metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix: \", metrics.confusion_matrix(y_test, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n",
    "print(\"Positive Predictive Value: \", metrics.precision_score(y_test, y_pred))\n",
    "print(\"True Positive Rate: \", metrics.recall_score(y_test, y_pred))\n",
    "print(\"True Negative Rate: Not directly available in sklearn.metrics\")\n",
    "print(\"False Positive Rate: Not directly available in sklearn.metrics\")\n",
    "fnr = 1 - metrics.recall_score(y_test, y_pred)\n",
    "print(\"False Negative Rate: \", fnr)\n",
    "print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_class_ydataprofiling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
